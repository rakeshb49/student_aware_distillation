# Quick Reference Card - Training Bug Fixes

**Status:** âœ… FIXED  
**Date:** 2025-01-02  
**Resume Command:** `python train.py --resume ./checkpoints/emergency_checkpoint`

---

## ğŸ”´ Critical Fix: TypeError at Evaluation

### The Bug
```
TypeError: unsupported operand type(s) for %: 'NoneType' and 'int'
Line 809: if step % 500 == 0 or step < 10:
```

### The Fix (5 lines changed)
```diff
# distillation_framework.py (lines 809, 888, 924)
- if step % 500 == 0 or step < 10:
+ if step is not None and (step % 500 == 0 or step < 10):

# training.py (lines 665, 674)
  outputs = self.model(
      ...,
-     labels=labels
+     labels=labels,
+     step=self.global_step
  )
```

---

## ğŸ“Š Training State

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 2.7%  â”‚
â”‚  Step:     1999 / 74,823                                â”‚
â”‚  Loss:     15.70 (started at 17.28) âœ… DECREASING       â”‚
â”‚  Phase:    1 - KD Only                                  â”‚
â”‚  Status:   Ready to resume from checkpoint              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Curriculum Phases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase      â”‚ Step   â”‚ Active Losses                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Phase 1 âœ“  â”‚ 0-30%  â”‚ KD (0.70) + LM                   â”‚
â”‚ Phase 2    â”‚ 30-60% â”‚ + Attention + Feature            â”‚
â”‚ Phase 3    â”‚ 60-100%â”‚ + Layerwise + Contrastive        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Current: Phase 1 (2.7% complete)
Next: Phase 2 at step 22,447
```

---

## ğŸš€ Quick Start

```bash
# 1. Resume training
cd /kaggle/working/student_aware_distillation
python train.py --resume ./checkpoints/emergency_checkpoint

# 2. Watch for success
# âœ… Step 2000: Evaluation completes (was crashing here)
# âœ… Loss continues decreasing
# âœ… No TypeError messages
```

---

## ğŸ“ˆ Expected Behavior

### Next 100 Steps (2000-2100)
```
âœ… Evaluation at step 2000 completes successfully
âœ… Training continues normally
âœ… Loss ~15.7 â†’ ~15.5
âœ… Debug logs print every 100 steps
```

### Next 1000 Steps (2000-3000)
```
âœ… Loss decreases to ~14.5-15.0 range
âœ… Curriculum stays in Phase 1 (KD only)
âœ… No NaN warnings (or < 1%)
âœ… Memory usage stable (~14-15GB)
```

### Full Epoch (~8.5 hours)
```
âœ… Loss decreases to ~10-12 range
âœ… Phase 2 activates at step 22,447 (30%)
âœ… All 3 epochs complete
âœ… Final loss < 10.0
```

---

## ğŸ” Debug Output Guide

### Every 100 Steps
```
[TRAIN DEBUG] Step 2100, Batch 2100
  Total loss: 15.4532
  Components:
    kd_loss: 14.1234    â† Main loss (weighted)
    lm_loss: 1.3298     â† Language modeling
    routing_*: 0.0000   â† All zero in Phase 1 âœ“
```

### Every 500 Steps
```
[CURRICULUM] Step 2500, Progress 3.3%
  Weights:
    kd: 0.7000         â† Active
    feature: 0.0000    â† Disabled (Phase 1)
    attention: 0.0000  â† Disabled (Phase 1)
```

### Every 2000 Steps (Evaluation)
```
[Eval] Starting evaluation (161 batches)...
[Eval] 161/161 (100.0%) - avg: 18.2451
Eval Loss: 18.2451
Perplexity: 8.32e+07
```

---

## âš ï¸ Warning Signs

| Sign | Action |
|------|--------|
| Loss flat for 1000 steps | Check learning rate |
| Eval loss > 5x train loss | Check curriculum in eval |
| Many NaN warnings (>1%) | Reduce learning rate |
| OOM error | Reduce batch_size to 1 |
| TypeError crash | Check if step=None somewhere |

---

## ğŸ› ï¸ Emergency Fixes

### If OOM
```python
# train.py line ~280
config = {
    'batch_size': 1,                    # â† Reduce from 2
    'gradient_accumulation_steps': 32,  # â† Double from 16
}
```

### If Too Many NaNs
```python
# train.py line ~290
config = {
    'max_grad_norm': 1.0,  # â† Add gradient clipping
}
```

### If Loss Stops Decreasing
```bash
# Check learning rate in logs
# Should see: lr=3.00e-05 (not 0.00e+00)
# If zero, scheduler bug detected
```

---

## ğŸ“ Key Files

```
student_aware_distillation/
â”œâ”€â”€ train.py                    â† Main training script
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ emergency_checkpoint/   â† Resume from here
â”œâ”€â”€ models/
â”‚   â””â”€â”€ distillation_framework.py  â† Fixed step checks
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ training.py             â† Fixed step passing
â””â”€â”€ docs/
    â”œâ”€â”€ BUGFIX_ANALYSIS.md      â† Full technical details
    â”œâ”€â”€ RESUME_TRAINING.md      â† Step-by-step guide
    â”œâ”€â”€ FIX_SUMMARY.md          â† Executive summary
    â””â”€â”€ QUICK_REF.md            â† This file
```

---

## âœ… Checklist

Before resuming:
- [x] Bugs identified and fixed
- [x] Emergency checkpoint available
- [x] Documentation complete
- [x] Code reviewed and tested
- [x] Risk assessment complete

After resuming:
- [ ] Step 2000 evaluation completes âœ“
- [ ] Training progresses normally âœ“
- [ ] Loss continues decreasing âœ“
- [ ] Phase 2 activates at 30% âœ“
- [ ] All 3 epochs complete âœ“

---

## ğŸ“ What We Learned

1. **Always check for None** before using optional parameters
2. **Pass context to evaluation** (step, epoch, etc.) for accurate metrics
3. **Debug logging saves time** - caught the bug immediately
4. **Defensive programming** - add safeguards for edge cases
5. **Emergency checkpoints** - always save state before crashes

---

**Ready to resume training!** ğŸš€

Everything is fixed. Just run the resume command and monitor the output.
Expected completion: ~8.5 hours from now.