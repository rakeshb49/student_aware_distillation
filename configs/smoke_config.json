{
  "comment": "Smoke-test config for Kaggle GPU runs (~90 minutes).",

  "teacher_model": "huihui-ai/Huihui-MoE-1B-A0.6B",
  "student_model": "HuggingFaceTB/SmolLM-135M",

  "batch_size": 2,
  "gradient_accumulation_steps": 4,
  "learning_rate": 3e-5,
  "router_lr": 1e-4,
  "num_epochs": 1,
  "warmup_steps": 200,
  "eval_steps": 250,
  "save_epochs": 1,
  "max_length": 320,

  "alpha_kd": 0.7,
  "alpha_feature": 0.1,
  "alpha_attention": 0.1,
  "alpha_layerwise": 0.05,
  "alpha_contrastive": 0.05,

  "temperature": 3.0,
  "min_temperature": 2.0,
  "use_temperature_curriculum": true,
  "contrastive_temp": 0.07,
  "label_smoothing": 0.1,
  "use_curriculum": true,

  "initial_top_k": 1,
  "final_top_k": 4,
  "load_balance_weight": 0.01,
  "noise_std": 0.1,

  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "scheduler_type": "cosine",
  "use_amp": true,
  "amp_dtype": "bfloat16",
  "loss_chunk_size": 128,
  "attention_layers": 2,

  "kd_top_k": 256,
  "use_gradient_checkpointing": true,

  "dataset_subset_size": 8000,
  "num_workers": 1,
  "use_dynamic_batching": true,

  "checkpoint_dir": "./checkpoints",
  "log_dir": "./logs",
  "cache_dir": "./cache",

  "memory_threshold": 0.85,

  "use_early_stopping": true,
  "early_stopping_patience": 6,
  "early_stopping_min_delta": 0.002,

  "use_ema": true,
  "ema_decay": 0.9999,

  "use_wandb": false,
  "project_name": "student-aware-distillation-smoke",
  "log_component_losses": true,
  "log_gradient_norms": true
}
