{
  "comment": "EMERGENCY FIX - Addresses all critical training issues identified in logs",

  "teacher_model": "huihui-ai/Huihui-MoE-1B-A0.6B",
  "student_model": "HuggingFaceTB/SmolLM-135M",
  "num_experts": 8,
  "top_k": 2,

  "_CRITICAL_FIX_1": "Warmup steps corrected for gradient accumulation",
  "batch_size": 2,
  "gradient_accumulation_steps": 16,
  "learning_rate": 3e-5,
  "router_lr": 1e-4,
  "num_epochs": 3,
  "warmup_steps": 500,
  "eval_steps": 2000,
  "save_epochs": 1,

  "_CRITICAL_FIX_2": "Reduced sequence length for memory stability",
  "max_length": 256,
  "teacher_max_length": 256,

  "_CRITICAL_FIX_3": "Loss weights balanced and normalized",
  "alpha_kd": 0.55,
  "alpha_feature": 0.1,
  "alpha_attention": 0.1,
  "alpha_layerwise": 0.05,
  "alpha_contrastive": 0.05,

  "_CRITICAL_FIX_4": "Temperature reduced for numerical stability",
  "temperature": 2.0,
  "min_temperature": 1.5,
  "use_temperature_curriculum": true,
  "contrastive_temp": 0.07,
  "label_smoothing": 0.1,

  "_CRITICAL_FIX_5": "Subset KD enabled for 10-100x speedup and memory savings",
  "kd_top_k": 256,
  "use_subset_kd": true,

  "use_curriculum": true,

  "initial_top_k": 1,
  "final_top_k": 4,
  "load_balance_weight": 0.01,
  "noise_std": 0.1,

  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "scheduler_type": "cosine",
  "use_amp": true,
  "amp_dtype": "bfloat16",
  "loss_chunk_size": 128,

  "_CRITICAL_FIX_6": "Reduced attention layers for memory efficiency",
  "attention_layers": 1,
  "use_adaptive_loss_balancing": true,
  "adaptive_balance_strength": 1.0,
  "adaptive_balance_min_multiplier": 0.15,
  "adaptive_balance_max_multiplier": 3.0,
  "adaptive_balance_epsilon": 0.0001,

  "_CRITICAL_FIX_7": "Memory optimization settings",
  "use_gradient_checkpointing": true,
  "memory_threshold": 0.85,

  "dataset_subset_size": 50000,
  "num_workers": 1,
  "use_dynamic_batching": true,

  "checkpoint_dir": "./checkpoints",
  "log_dir": "./logs",
  "cache_dir": "./cache",

  "_CRITICAL_FIX_8": "More patient early stopping",
  "use_early_stopping": true,
  "early_stopping_patience": 10,
  "early_stopping_min_delta": 0.001,

  "_CRITICAL_FIX_9": "EMA enabled for stable evaluation",
  "use_ema": true,
  "ema_decay": 0.9999,

  "use_wandb": false,
  "project_name": "student-aware-distillation",
  "log_component_losses": true,
  "log_gradient_norms": true,

  "_EXPECTED_RESULTS": {
    "note": "After applying all fixes",
    "initial_loss": "20-30",
    "end_epoch_1_loss": "8-15",
    "end_epoch_1_perplexity": "50-100",
    "training_speed": "3-5 it/s with subset KD",
    "memory_usage": "80-85%",
    "time_per_epoch": "1.5-2 hours"
  },

  "_CRITICAL_FIXES_SUMMARY": [
    "1. Scheduler step calculation fixed in utils/training.py",
    "2. Warmup steps: 500 optimizer steps = ~13% of training",
    "3. Subset KD enabled (kd_top_k=256) for massive speedup",
    "4. Temperature reduced to 2.0 to prevent NaN",
    "5. Sequence length reduced to 256 for memory",
    "6. Attention layers reduced to 1 for memory",
    "7. Evaluation frequency reduced to 2000 steps",
    "8. Early stopping patience increased to 10",
    "9. All loss components properly balanced"
  ]
}
